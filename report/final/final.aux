\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*{\memsetcounter}[2]{}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\abx@aux@refcontext{nty/global//global/global}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@cite{Classic_Control_Theory}
\abx@aux@segm{0}{0}{Classic_Control_Theory}
\abx@aux@cite{Deep_Q_Learning_First}
\abx@aux@segm{0}{0}{Deep_Q_Learning_First}
\abx@aux@cite{Sutton_Barto}
\abx@aux@segm{0}{0}{Sutton_Barto}
\abx@aux@cite{Deep_Q_Learning_First}
\abx@aux@segm{0}{0}{Deep_Q_Learning_First}
\abx@aux@cite{Double_Pendulum_Equations}
\abx@aux@segm{0}{0}{Double_Pendulum_Equations}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.}Introduction}{1}{section.0.1}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A double pendulum being controlled by a trained agent that is applying $-1N$ in the horizontal direction.\relax }}{1}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:double_pendulum_controlled}{{\M@TitleReference {1}{A double pendulum being controlled by a trained agent that is applying $-1N$ in the horizontal direction.\relax }}{1}{A double pendulum being controlled by a trained agent that is applying $-1N$ in the horizontal direction.\relax }{figure.caption.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Reinforcement Learning}{1}{subsection.0.1.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.}Modeling}{1}{section.0.2}\protected@file@percent }
\abx@aux@cite{Double_Pendulum_Equations}
\abx@aux@segm{0}{0}{Double_Pendulum_Equations}
\abx@aux@cite{original_q_learning}
\abx@aux@segm{0}{0}{original_q_learning}
\abx@aux@cite{Deep_Q_Learning_First}
\abx@aux@segm{0}{0}{Deep_Q_Learning_First}
\newlabel{eq_rhs_double_pendulum}{{2}{2}{Modeling}{equation.0.2.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.}System Design}{2}{section.0.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Q-learning}{2}{subsection.0.3.1}\protected@file@percent }
\newlabel{eq:exp}{{7}{2}{Q-learning}{equation.0.3.7}{}}
\newlabel{eq:qlearning}{{8}{2}{Q-learning}{equation.0.3.8}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Deep Q-learning}{2}{subsection.0.3.2}\protected@file@percent }
\newlabel{eqn:neural_network_mapping}{{9}{2}{Deep Q-learning}{equation.0.3.9}{}}
\abx@aux@cite{Deep_Q_Learning_First}
\abx@aux@segm{0}{0}{Deep_Q_Learning_First}
\abx@aux@cite{Deep_Q_Learning_Target_Network}
\abx@aux@segm{0}{0}{Deep_Q_Learning_Target_Network}
\newlabel{fig:q_network_illustration}{{\M@TitleReference {\caption@xref {fig:q_network_illustration}{ on input line 267}}{Deep Q-learning}}{3}{Deep Q-learning}{figure.caption.2}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Illustration of a neural network with $3$ state inputs and $4$ Q-value outputs that can be mapped to actions.\relax }}{3}{figure.caption.2}\protected@file@percent }
\newlabel{alg:deep_q_training}{{\M@TitleReference {\caption@xref {alg:deep_q_training}{ on input line 294}}{Deep Q-learning}}{3}{Deep Q-learning}{equation.0.3.10}{}}
\@writefile{loa}{\defcounter {refsection}{0}\relax }\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Deep Q-Learning\relax }}{3}{algorithm.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.}Implementation}{3}{section.0.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Simulator}{3}{subsection.0.4.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Environment}{3}{subsection.0.4.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Evaluation}{3}{subsection.0.4.3}\protected@file@percent }
\abx@aux@cite{Correct_Equations}
\abx@aux@segm{0}{0}{Correct_Equations}
\abx@aux@cite{He_initialization}
\abx@aux@segm{0}{0}{He_initialization}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Code architecture.\relax }}{4}{figure.caption.3}\protected@file@percent }
\newlabel{fig:code_architecture}{{\M@TitleReference {3}{Code architecture.\relax }}{4}{Code architecture.\relax }{figure.caption.3}{}}
\newlabel{eqn:evaluation_equation}{{12}{4}{Evaluation}{equation.0.4.12}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.}Results}{4}{section.0.5}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Balancing Inverted Single Pendulum}{4}{subsection.0.5.1}\protected@file@percent }
\newlabel{eqn:reward_single_pendulum}{{13}{4}{Balancing Inverted Single Pendulum}{equation.0.5.13}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Network parameters for the single pendulum Q-network.\relax }}{4}{table.caption.4}\protected@file@percent }
\newlabel{table:params_q_network}{{\M@TitleReference {1}{Network parameters for the single pendulum Q-network.\relax }}{4}{Network parameters for the single pendulum Q-network.\relax }{table.caption.4}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Parameters for the Deep Q-learning algorithm on the single pendulum problem.\relax }}{4}{table.caption.5}\protected@file@percent }
\newlabel{table:params_pendulum}{{\M@TitleReference {2}{Parameters for the Deep Q-learning algorithm on the single pendulum problem.\relax }}{4}{Parameters for the Deep Q-learning algorithm on the single pendulum problem.\relax }{table.caption.5}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Evaluated controller score for every $10$ episodes. The evaluation is measured as the average score of $10$ full controlled simulations. Maximum number of steps for each evaluation iteration was $200$, capping the maximum possible reward to $200$.\relax }}{4}{figure.caption.6}\protected@file@percent }
\newlabel{fig:single_pendulum_eval}{{\M@TitleReference {4}{Evaluated controller score for every $10$ episodes. The evaluation is measured as the average score of $10$ full controlled simulations. Maximum number of steps for each evaluation iteration was $200$, capping the maximum possible reward to $200$.\relax }}{4}{Evaluated controller score for every $10$ episodes. The evaluation is measured as the average score of $10$ full controlled simulations. Maximum number of steps for each evaluation iteration was $200$, capping the maximum possible reward to $200$.\relax }{figure.caption.6}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Changes in pendulum angle $\theta $ over time for controlled simulation of single pendulum on cart problem. The initial condition was $\theta = 35$ degrees.\relax }}{5}{figure.caption.7}\protected@file@percent }
\newlabel{fig:single_pendulum_outside_training_domain}{{\M@TitleReference {5}{Changes in pendulum angle $\theta $ over time for controlled simulation of single pendulum on cart problem. The initial condition was $\theta = 35$ degrees.\relax }}{5}{Changes in pendulum angle $\theta $ over time for controlled simulation of single pendulum on cart problem. The initial condition was $\theta = 35$ degrees.\relax }{figure.caption.7}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Q-values for $\gamma =0.1$.\relax }}{5}{figure.caption.8}\protected@file@percent }
\newlabel{fig:pendulum_gamma_01}{{\M@TitleReference {6}{Q-values for $\gamma =0.1$.\relax }}{5}{Q-values for $\gamma =0.1$.\relax }{figure.caption.8}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Q-values for $\gamma =0.9$.\relax }}{5}{figure.caption.9}\protected@file@percent }
\newlabel{fig:pendulum_gamma_09}{{\M@TitleReference {7}{Q-values for $\gamma =0.9$.\relax }}{5}{Q-values for $\gamma =0.9$.\relax }{figure.caption.9}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Evaluation of the controller with the same conditions in Table \ref  {table:params_pendulum}, except that $\gamma =0.1$.\relax }}{5}{figure.caption.10}\protected@file@percent }
\newlabel{fig:single_pendulum_eval_01}{{\M@TitleReference {8}{Evaluation of the controller with the same conditions in Table \ref  {table:params_pendulum}, except that $\gamma =0.1$.\relax }}{5}{Evaluation of the controller with the same conditions in Table \ref {table:params_pendulum}, except that $\gamma =0.1$.\relax }{figure.caption.10}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The distribution of used actions for an action space of $[-20,-10,-5,0,5,10,20]$. The model was trained with the same hyperparameters in Table \ref  {table:params_pendulum}.\relax }}{5}{figure.caption.11}\protected@file@percent }
\newlabel{fig:pendulum_force}{{\M@TitleReference {9}{The distribution of used actions for an action space of $[-20,-10,-5,0,5,10,20]$. The model was trained with the same hyperparameters in Table \ref  {table:params_pendulum}.\relax }}{5}{The distribution of used actions for an action space of $[-20,-10,-5,0,5,10,20]$. The model was trained with the same hyperparameters in Table \ref {table:params_pendulum}.\relax }{figure.caption.11}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Balancing Inverted Double Pendulum}{5}{subsection.0.5.2}\protected@file@percent }
\newlabel{eqn:termination_double_pendulum}{{14}{5}{Balancing Inverted Double Pendulum}{equation.0.5.14}{}}
\newlabel{eqn:reward_double_pendulum}{{15}{6}{Balancing Inverted Double Pendulum}{equation.0.5.15}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Network parameters for the double pendulum Q-network.\relax }}{6}{table.caption.12}\protected@file@percent }
\newlabel{table:double_params_q_network}{{\M@TitleReference {3}{Network parameters for the double pendulum Q-network.\relax }}{6}{Network parameters for the double pendulum Q-network.\relax }{table.caption.12}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Parameters for the Deep Q-learning algorithm on the double pendulum problem.\relax }}{6}{table.caption.13}\protected@file@percent }
\newlabel{table:params_double_pendulum}{{\M@TitleReference {4}{Parameters for the Deep Q-learning algorithm on the double pendulum problem.\relax }}{6}{Parameters for the Deep Q-learning algorithm on the double pendulum problem.\relax }{table.caption.13}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Evaluated controller score for every 10 episodes. The evaluation is measured as the average score of 10 full controlled simulations. The baseline is measured by evaluating 200 simulations without a controller which lasted on average for $0.35$ seconds. A controller that performs actions randomly survives an average time of $0.28$ seconds, estimated the same way as the baseline.\relax }}{6}{figure.caption.14}\protected@file@percent }
\newlabel{fig:double_pendulum_score}{{\M@TitleReference {10}{Evaluated controller score for every 10 episodes. The evaluation is measured as the average score of 10 full controlled simulations. The baseline is measured by evaluating 200 simulations without a controller which lasted on average for $0.35$ seconds. A controller that performs actions randomly survives an average time of $0.28$ seconds, estimated the same way as the baseline.\relax }}{6}{Evaluated controller score for every 10 episodes. The evaluation is measured as the average score of 10 full controlled simulations. The baseline is measured by evaluating 200 simulations without a controller which lasted on average for $0.35$ seconds. A controller that performs actions randomly survives an average time of $0.28$ seconds, estimated the same way as the baseline.\relax }{figure.caption.14}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Change in pendulum angles controlled by trained agent for initial conditions set as $\theta _1=\theta _2=-20^{\circ }$.\relax }}{6}{figure.caption.15}\protected@file@percent }
\newlabel{fig:double_pendulum_trained_angles}{{\M@TitleReference {11}{Change in pendulum angles controlled by trained agent for initial conditions set as $\theta _1=\theta _2=-20^{\circ }$.\relax }}{6}{Change in pendulum angles controlled by trained agent for initial conditions set as $\theta _1=\theta _2=-20^{\circ }$.\relax }{figure.caption.15}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Training Time}{6}{subsection.0.5.3}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Force distribution for the trained agent controlling the pendulums with initial conditions of $\theta _1=\theta _2=-20^{\circ }$.\relax }}{7}{figure.caption.16}\protected@file@percent }
\newlabel{fig:double_pendulum_force_dist}{{\M@TitleReference {12}{Force distribution for the trained agent controlling the pendulums with initial conditions of $\theta _1=\theta _2=-20^{\circ }$.\relax }}{7}{Force distribution for the trained agent controlling the pendulums with initial conditions of $\theta _1=\theta _2=-20^{\circ }$.\relax }{figure.caption.16}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Computation and simulation times for episodes with hyperparameters specified in Table \ref  {table:params_pendulum}.\relax }}{7}{table.caption.17}\protected@file@percent }
\newlabel{table:def_params_time}{{\M@TitleReference {5}{Computation and simulation times for episodes with hyperparameters specified in Table \ref  {table:params_pendulum}.\relax }}{7}{Computation and simulation times for episodes with hyperparameters specified in Table \ref {table:params_pendulum}.\relax }{table.caption.17}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Computation and simulation times for episodes with hyperparameters specified in Table \ref  {table:params_pendulum}, with a modification of $\theta _{max} = 20$ degrees.\relax }}{7}{table.caption.18}\protected@file@percent }
\newlabel{table:other_params_time}{{\M@TitleReference {6}{Computation and simulation times for episodes with hyperparameters specified in Table \ref  {table:params_pendulum}, with a modification of $\theta _{max} = 20$ degrees.\relax }}{7}{Computation and simulation times for episodes with hyperparameters specified in Table \ref {table:params_pendulum}, with a modification of $\theta _{max} = 20$ degrees.\relax }{table.caption.18}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Computation and simulation times for episodes with hyperparameters specified in Table \ref  {table:params_double_pendulum}.\relax }}{7}{table.caption.19}\protected@file@percent }
\newlabel{table:def_params_time_double}{{\M@TitleReference {7}{Computation and simulation times for episodes with hyperparameters specified in Table \ref  {table:params_double_pendulum}.\relax }}{7}{Computation and simulation times for episodes with hyperparameters specified in Table \ref {table:params_double_pendulum}.\relax }{table.caption.19}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.}Discussion}{7}{section.0.6}\protected@file@percent }
\abx@aux@cite{Linear_Q_Learning}
\abx@aux@segm{0}{0}{Linear_Q_Learning}
\abx@aux@cite{Deep_Double_Q_Learning}
\abx@aux@segm{0}{0}{Deep_Double_Q_Learning}
\abx@aux@read@bbl@mdfivesum{70B88DFF981675B23219860F6E5EE6F7}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{Correct_Equations}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{Deep_Double_Q_Learning}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{He_initialization}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{Deep_Q_Learning_Target_Network}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{Deep_Q_Learning_First}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{Classic_Control_Theory}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{Double_Pendulum_Equations}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{Sutton_Barto}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{original_q_learning}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{Linear_Q_Learning}{nty/global//global/global}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{References}{8}{section*.21}\protected@file@percent }
\memsetcounter{lastsheet}{8}
\memsetcounter{lastpage}{8}
\gdef \@abspage@last{8}
