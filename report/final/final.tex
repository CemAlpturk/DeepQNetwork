% !TeX encoding = UTF-8

\documentclass{LTHtwocol} % Use this when you work on your report.
% \documentclass[final]{LTHtwocol} % Use this for the final version.
                                   % It will remove page numbers and
                                   % markers for overfull boxes.
                                   % There really shouldn't be any of those anyway.

\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,graphicx}
\usepackage{float}
\usepackage[table,xcdraw]{xcolor}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{subcaption}
\hypersetup{
	colorlinks,
	linkcolor={red!50!black},
	citecolor={blue!50!black},
	urlcolor={blue!80!black}
}

\usepackage{kantlipsum} % Only for the dummy text. Remove for your own report.

\addbibresource{bibliography.bib}

% Custom commands
% From: https://tex.stackexchange.com/questions/130307/questions-about-algpseudocode
\algnewcommand\True{\textbf{true}\space}
\algnewcommand\False{\textbf{false}\space}

% Document begins here
\begin{document}
\begin{frontmatter}
\title{Balancing a Double Inverted Pendulum on Cart} % Title of the project.
                      							 % Note that all reports are in English,
							                     %so that our international students can read them.

\author[cem]{Cem Alpt\"urk}
\author[pukashawar]{Pukashawar Pannu}

\email[cem]{ce5368al-s@student.lu.se}
\email[pukashawar]{pu6218pa-s@student.lu.se}

\begin{abstract}
    Controlling a double inverted pendulum on a cart is an inherently difficult problem.
    The system is chaotic meaning that the dynamics cannot be predicted over a longer time span.
    A way to control such a system is to use the current state to determine an action that moves the cart appropriately to balance the double pendulum.
    One such way is by reinforcement learning.
    The approach in this project uses a model free controller trained with Deep Q-learning to control the double pendulum.
    Benefits of Deep Q-learning over regular Q-learning is that the controller can be used for situations that are not presented during training.
    This has been confirmed in the results for the single pendulum model, an easier problem to control that was used to confirm the implementation of the learning algorithm.
    The double pendulum is more difficult with its chaotic nature and the limitation that the cart with a single degree of freedom is trying to control multiple states.
    Results show that the double pendulum can be balanced for up-to one second with a network of $5$ layers and $20$ nodes in each layer.
    The agent was trained for a limited number of episodes limiting the amount of experience and knowledge it could gather about the problem.
    We believe that by tuning hyperparameters, network size and training one could train an agent to balance the double pendulum for far longer using Deep Q-Learning.
    % The abstract should be a 200--250 word compact description of your project. What was the objective? Which methods did you use? What was the (main) result?
\end{abstract}

\end{frontmatter}

% Stick to the proposed structure below. Add \subsections{} as appropriate.
% This file compiles on the Automatic Control Department system by typing the
% following into the terminal (while in the directory of the file, and with all
% other files belonging to the template untouched):
% > pdflatex template
% > biber template
% The first line compiles the .tex file. The second line generates the
% bibliography. Once this is done, you may need to run the first line 1-2
% additional times, for the system to get all cross references right in the
% produced pdf output.

\section{Introduction}
The goal of this project is to teach an agent to control a double pendulum on a cart to be standing upright, in its equilibrium point, using reinforcement learning.
A double pendulum is a highly chaotic system, where its behavior heavily depends on its initial conditions.
Controlling the system with a cart limited to a single degree of freedom is a difficult but solvable problem \cite{Classic_Control_Theory}.
Figure \ref{fig:double_pendulum_chaotic} shows a simulation with 4 double pendulums with nearly identical initial conditions.
After some time their behaviors become so different that it becomes nearly impossible to make a prediction about the future.
This illustrates the chaotic behavior of this system.
The aim is to create an agent that can learn from its own experiences to control the double pendulum, by applying a force on the cart in the horizontal direction, such that it stays upright.
Although this is an academic problem, the results could be applied to any type of learning problem since the agent is unaware of what it's working on \cite{Deep_Q_Learning_First}.

% Here you introduce the project. What is the background? What project do you aim at solve? References to prior work? If the project makes a positive or negative environmental, or other solitary, impact, describe it here. Are there any ethical considerations? You might want to reference relevant literature, e.g. \cite{openclosed2, Hellerstein2004, Yun2015}. A general \LaTeX\ guide is available at \cite{latexwiki}.

\subsection{Reinforcement Learning}
%\kant[1] % This generates the dummy text
The main idea of reinforcement learning is to learn to make decisions based on past experiences.
In this case this means to train an agent to perform the best action possible in order to control the pendulums.
For each action taken by the agent, the environment returns a new state and a reward for that action.
The reward is calculated based on how close the system is to its equilibrium point.
The goal of training the agent is to optimize gained reward for each action taken.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\columnwidth]{figures/Double_Pendulum_Chaotic.PNG}
	\caption{4 double pendulums simulated with similar initial conditions after $9.86$ seconds, with a difference of $0.02 rad/s$ differences on the inner pendulum angular velocities. All other initial states were equal.}
	\label{fig:double_pendulum_chaotic}
\end{figure}

\section{Modeling}
The system consists of a cart that can move in the horizontal direction and two pendulums that are coupled to each other such that there is no collision between any parts.
Since this project is not about modeling the double pendulum, but rather about controlling it, the equations of motion have been borrowed from \cite{Double_Pendulum_Equations}.
The states of the system are represented by,
\begin{equation}
x  :=
\begin{bmatrix}
y & \dot{y}
\end{bmatrix}
^T , \quad y :=
\begin{bmatrix}
q & \theta_1 & \theta_2
\end{bmatrix}
^T
\end{equation}
%\begin{equation}
%x :=
%\begin{bmatrix}
%q & \theta_1 &  \theta_2 &  \dot{q} &  \dot{\theta_1} & \dot{\theta_2}
%\end{bmatrix}
% ^T
%\end{equation}
\begin{itemize}
\item $q$ is the position of the cart on the horizontal axis.
\item $\theta_1$ is the angle of the inner pendulum with respect to the vertical axis in the clockwise direction.
\item $\theta_2$ is the angle of the outer pendulum with respect to the vertical axis in the clockwise direction.
\item $\dot{q}$ is the velocity of the cart in the horizontal axis.
\item $\dot{\theta_1}$ is the angular velocity of the inner pendulum.
\item $\dot{\theta_2}$ is the angular velocity of the outer pendulum.
\end{itemize}

The system parameters are represented by,
\begin{itemize}
\item $m = 1 [kg]$ is the mass of the cart.
\item $m_1 = 0.1 [kg]$ is the mass of the inner pendulum.
\item $m_2 = 0.1[kg]$ is the mass of the outer pendulum.
\item $l_1 = 1 [m]$ is the length of the inner pendulum.
\item $l_2 = 1[m]$ is the length of the outer pendulum.
\item $g = 9.81 [m/s^2]$ is the gravitational acceleration.
 \end{itemize}

The equations of motions for this system \cite{Double_Pendulum_Equations}, are adjusted such there is no friction or disturbances.
The input to the system is a horizontal force that is acting on the cart, represented by $u$ $[N]$.

\begin{equation}
M(y) \ddot{y} = f(y,\dot{y},u)
\end{equation}


\begin{equation}
\resizebox{ \columnwidth}{!}
{
$
M(y) :=
\begin{bmatrix}
m + m_1 +m_2 & l_1(m_1+m_2)\cos \theta_1 & m_2 l_2 \cos \theta_2 \\
l_1 (m_1 + m_2) \cos \theta_1 & l_1^2(m_1+m_2) & l_1 l_2 m_2 \cos(\theta_1 - \theta_2) \\
l_2 m_2 \cos \theta_2 & l_1 l_2 m_2 \cos (\theta_1 - \theta_2) & l_2^2 m_2
\end{bmatrix}
$
}
\end{equation}

\begin{equation}
\resizebox{ \columnwidth}{!}
{
$
f(y,\dot{y},u) :=
\begin{bmatrix}
l_1(m_1+m_2) (\dot{\theta_1})^2 \sin \theta_1 + m_2 l_2 (\dot{\theta_2})^2 \sin \theta_2 + u\\
-l_1l_2m_2(\dot{\theta_2})^2 \sin(\theta_1 - \theta_2) + g(m_1 + m_2)l_1 \sin \theta_1 \\
l_1l_2m_2(\dot{\theta_1})^2 \sin (\theta_1 - \theta_2) + g l_2 m_2 \sin \theta_2
\end{bmatrix}
 $
 }
\end{equation}






%Here you present the modeling approach and publish your model. If your model has 63434 parameters, you may not wish to print it in detail. The idea is, however, that another group with your background should be able to reproduce your work -- this goes not only for the modeling aspect.
%
%If you use equations, make sure they are all numbered:
%\begin{equation}
%\alpha_a^2 + \beta_b^2 = \gamma_c^2.
%\label{eq:formula}
%\end{equation}
%Equations are parts of the text. If they end a sentence, they should end with a dot. If they end a clause, they should end with a comma. You refer to an equation this like: see \eqref{eq:formula}. Note that all units are written in roman type: $\omega=2\pi$~rad/s, $g = 9.81$~m/s$^2$. See \cite{mathslatexwiki} for a tutorial on typesetting maths.
%
%\subsection{Some Dummy Text}
%\kant[2]

\section{System Design}
A couple of reinforcement learning methods are Q-learning and Deep Q-learning.
\subsection{Q-learning}
Q-learning is a model free method \cite{original_q_learning} where the agent has no information about the system its trying to control, thus it can be said that it is a black box model.
The agent stores all its experiences in a Q-table that contains the quality of a state-action combination.
\begin{equation}
Q : S \times A  \to \mathbb{R}
\end{equation}
%\[ Q : S \times A  \to \mathbb{R} \]
\[Q(s,a) := \text{Quality of action $a$ given state  $s$} \]
A trained agent decides on the action to perform by looking up the best action $a$ given state $s$ from the Q-table.
In the case where multiple actions give the same values in the Q-table, the action is selected randomly among those.
The selected action is performed on the environment, which is the inverted pendulum, and a reward is calculated based on the new state of the system.
In the case where there is no previous knowledge about the current state in the Q-table, the action is selected randomly.
Essentially the agent tries to find the action that will give the best reward.
\begin{equation}
 a^* = \max_i Q(s,a_i)
\end{equation}
%\[ a^* = \max_i Q(s,a_i) \]
The agent uses an $\epsilon$-greedy policy to decide on the next move, where the agent performs a random action with probability $\epsilon$ or performs the best action based on its current policy with probability $1-\epsilon$.
Having a high $\epsilon$ value will correspond to performing randomly and exploring the state-action space but will delay the convergence of the Q-table. 
Having a low $\epsilon$ value can cause the agent to rarely try new things but instead will try to exploit its current solution to the problem.
A balance between these problems can be obtained by starting with a high $\epsilon$ value and decaying it over time to allow the agent to try new things in the beginning and slowly exploit the best solution it has found. 

Each state-action pair in a trained Q-table, represents the expected value in \eqref{eq:exp}. These values represent the expected return of rewards for future actions if action $a_t$ is taken at state $s_t$.
\begin{equation}
Q(s_t,a_t) = E[r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \hdots | s_t,a_t] 
\label{eq:exp}
\end{equation}
Training the agent corresponds to populating and updating the Q-table based on past experiences of the agent.
After performing an action given the current state of the system, the agent receives a reward from the environment and the values in the Q-table are updated according to the state-action combination and the reward received.
Equation \eqref{eq:qlearning} is known as the Bellman equation.
\begin{equation}
Q^{new}(s_t,a_t) \leftarrow (1 - \alpha)Q(s_t,a_t) + \alpha (r_t + \gamma \max_i Q(s_{t+1},a_i )) \\
\label{eq:qlearning} 
\end{equation}
\[ 0 < \alpha \leq 1, \quad 0 \leq \gamma \leq 1  \]

%\[ Q(s_t,a_t) = E[r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \hdots | s_t,a_t] \]
After performing action $a_t$ given the state $s_t$, the agent receives the new state $s_{t+1}$ and the reward $r_t$. The value $Q(s_t,a_t)$ is updated by \eqref{eq:qlearning}. $\alpha$ is the learning rate and $\gamma$ is the discount factor. Since the reward for a certain action might not come instantly, the discount factor tries to compensate for the delayed reward by estimating the reward that the agent will receive if it takes the best action $a_{t+1}$ for the state $s_{t+1}$. Low values of $\gamma$ will cause the agent to only consider near future rewards, while a $\gamma$ value close to 1 will consider future rewards.


This method requires a discrete set of actions and states. For the double pendulum problem, this can be achieved by discretizing the states and actions.
However since this is a chaotic system, it would require a fine discretization resulting in a massive Q-table. Although this can work in theory, there are practical implementation limitations such as limited memory size.
This problem can be resolved by using the Deep Q-learning method.

\subsection{Deep Q-learning}
In Deep Q-learning, the Q-table is replaced with a deep neural network \cite{Deep_Q_Learning_First} that tries to learn Q-values instead of memorizing them.
A benefit of using Deep Q-learning is that is uses less memory and it has more predictive power on unseen states.
The network takes in the system states as input arguments and predicts the q-values.
\begin{equation}
	\label{eqn:neural_network_mapping}
	NN_Q := \mathbb{R}^m \to \mathbb{R}^n
\end{equation}
where $m$ is the state size and $n$ is the size of the action space.
The agent chooses the action to perform by picking the largest q-value from the neural network prediction.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\columnwidth]{figures/q_network.png}
	\label{fig:q_network_illustration}
	\caption{Illustration of a neural network with $3$ state inputs and $4$ q-value outputs that can be mapped to actions.}
\end{figure}


The neural network is trained by a method called experience replay that is based on simulating the system and recording all states, rewards and actions taken in memory.
During experience replay, a random batch is taken from the memory and is used to train the network.
The reason for taking random batches from memory and not sequential data, is to reduce the correlation between the data points.
The simulation ends when it reaches a termination condition that is defined in the environment.
For the double pendulum case the termination condition is when the outer pendulum angle becomes larger than a defined maximum value.
One full simulation from start to termination is called an episode.
The stored experience is used to fit the neural network once there is enough experience stored in the memory.
\begin{equation}
	Exp \leftarrow \begin{pmatrix}s_t, & a_t, & r_t, & T_t, & s_{t+1}	\end{pmatrix}
\end{equation}
where $s_t$ is the state at time t, $a_t$ the action taken by the system at time t, $r_t$ the gained reward for the taken action, $T_t$ indicating whether the system has terminated and $s_{t+1}$ the next state in time.
The experience is used to map a target for the neural network training.
Targets are q-values for each action given a state.
Since the experience only holds data about the actual action that was taken for a given state the other q-values for the same state have to be estimated to complete the entire target vector.
There are several ways to estimate the unknown q-values for a given state.
Although the $NN_Q$ network can be used to predict the target values for the other actions it causes the targets to be non-stationary leading to the network chasing its own tail.
A separate neural network, $NN_{target}$, was used to avoid the non-stationary targets problem.
$NN_{target}$ has the same network architecture as $NN_Q$.
After a number of episodes the weights of $NN_Q$ are mapped to $NN_{target}$ to give better estimations of the unknown q-values.


\begin{algorithm}[H]
	\label{alg:deep_q_training}
	\caption{Deep Q-Learning}
	\begin{algorithmic}
		\For{episode $[0, \text{max episodes}]$ }
			\While{$t < t_{max}$ and $T_t$ is \False} \Comment{Simulate system}
				\State $q_{values} \gets NN_Q(s_t)$  % \Comment{Predict q-values for actions for current state}
				\State $a \gets argmax(q_{values})$ 	 \Comment{$a$ is index}
				\State $s_{t+1}, r_t, T_t \gets \text{Step}(ActionSpace[a])$ % \Comment{Simulate system one step}
				\State $Exp \gets \begin{pmatrix} s_t, & a_t, & r_t & T_t & s_{t+1} \end{pmatrix}$
				\State $\begin{pmatrix} \mathbf{s}, & \mathbf{a}, & \mathbf{r} & \mathbf{T} & \mathbf{s}_{t+1} \end{pmatrix}$ $\gets Exp$  \Comment{Exp minibatch}
				\State $y_m = \begin{cases} r_m \\ r_m + \gamma \max ( NN_{target}(\mathbf{s}_{t+1, m}) ) \end{cases}$
				%\State $Targets_{m,j} \gets \begin{cases} y_m, & \text{if} \quad j = a  \\ NN_Q(\mathbf{s}_m)_j \end{cases}$
				\State  $Loss \gets (y_m - NN_Q(\mathbf{s}_{t+1,m})_{a_m})^2$
				\State Perform gradient descent on Loss
				%\State $model.fit(\mathbf{s}, Targets)$
			\EndWhile
		\EndFor
	\end{algorithmic}
\end{algorithm}
The actual value $y_m$ replaces the corresponding predicted q-value from $NN_{target}(\mathbf{s}_m)$.
 Since we only have the reward for one action in a given state, we replace the targets for the untaken actions with a prediction from the Q-network. This will result in a loss of 0 for these untaken actions, thus not affecting the training.
 This way the network is trained only by the state-action pair that has been taken.
 This will however cause the implementation to be slower since a prediction has to be made before each weight update.
 This was solved by implementing a custom loss function in \texttt{Keras} where the loss only depends on the Q-value of the taken action.

% \comment{Predict q-values for actions for current state}
% \comment{Pick largest q value and map it to the action}
% \comment{Simulate system one step}

% \begin{equation}
% 	\label{eqn:termination_condition}

% \end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
% TODO
% * Termination condition - defined in the environment (simulation)
% * Bellman equation (5)
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%




%Describe the control or learning system you have designed.
%Did you build anything? If so, what did you build, and using what production methods. If you built the hardware or were handed it, \emph{a photograph of your gadget is mandatory}. Make sure any figures are referenced from from the text---like this, see Figure~\ref{fig:gadget}---and that they all have a descriptive caption.
%\begin{figure}[b]
%	\centering
%	\includegraphics[width=0.7\columnwidth]{balanduino}
%	\caption{Example picture of the Balanduino robot. Place all figures at the top \texttt{[t]} (default) or bottom \texttt{[b]} (only if needed).}
%	\label{fig:gadget} % Should be placed after the caption!
%\end{figure}
%
%\subsection{Some Dummy Text}
%\kant[3]

\section{Implementation}
The project was implemented in \texttt{Python}.
A separate class was created for each component \texttt{Agent}, \texttt{Environment}, \texttt{Simulator} and \texttt{Controller}.
The goal of the project is to produce a \texttt{Controller} object that can control the \texttt{Simulator}.
\texttt{Environment} class contains the problem specifics such as \texttt{reward} and \texttt{termination} functions, and links the \texttt{Agent} with the \texttt{Simulator} containing the physical model dynamics.
\texttt{Agent} contains the learning algorithm that trains and produces the \texttt{Controller}.
The \texttt{Controller} is a simple object that produces the force to apply given the current state of the system.
\begin{equation}
	C := \mathbb{R}^M \to \mathbb{R}
\end{equation}

All components are independent of each other making the structure modular.
By using this structure it is easy to change for example the learning algorithm without having to modify anything else.

\subsection{Simulator}
The \texttt{Simulator} is responsible for the physical dynamics of the problem and for simulating the problem for given external inputs and time.
Equations are on the form of RHS-equations and are solved using \texttt{odeint} function in \texttt{Scipy.integrate} package.

\subsection{Environment}
The reward function and termination condition is implemented in the \texttt{Environment} class.
It acts as a connector between the \texttt{Agent} and the \texttt{Simulator}.
This way when the reward or the termination needs to be changed it can be done without altering the learning algorithm (\texttt{Agent}) or the physical model \texttt{Simulator}.

% Reward function : only look at the angle, not the velocity
% 					don't want the algorithm to sacrifies angle for velocity ... something like that.

% Here you describe how you implemented your learning or control design.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\columnwidth]{figures/CodeStructure.png}
	\label{fig:code_architecture}
	\caption{Code architecture.}
\end{figure}

\subsection{Evaluation}
\texttt{Controller} is evaluated after a set of episodes in order to keep track of the controller performance.
Performance is measured by averaging the total reward for $N_{eval}$ simulations of the dynamic system controlled by the \texttt{Controller}.
\begin{equation}
	\label{eqn:evaluation_equation}
	\frac{1}{N_{eval}} \sum_i^{N_{eval}} \sum_j^{M_i} R(s_j)
\end{equation}
Notice that the number of simulation steps $M_i$ for each simulation varies as a simulation may reach a termination condition sooner or later than another simulation.
% $\mathbf{R}_i$ is the total reward for the $i$th simulation of the controlled dynamic system.
A satisfactory evaluation value can be used as a termination condition for the training to avoid overtraining.
This mechanism has not yet been implemented.
The training will continue until the max iterations have been reached.



\section{Results}
A simpler problem was used to confirm the implementation.
The simpler problem being to balance a single inverted pendulum on a cart.
The implementation architecture was used to simply replace the environment with one suitable for the single pendulum problem.
% Once satisfied with the algorithm implementation the double pendulum problem was run.

\subsection{Balancing Single Inverted Pendulum}
The single pendulum problem is non-chaotic.
Equations from \cite{Correct_Equations} were used to model the problem.
System was terminated when the pendulum angle was greater than $\pm 10$ degrees.
Equation \eqref{eqn:reward_single_pendulum} states the reward function.

\begin{equation}
    \label{eqn:reward_single_pendulum}
    R(s) = (\theta_{max} - |\theta|)/ \theta_{max}
\end{equation}
where $\theta_{max} = 10$ degrees.
The value was chosen by trial and error, where $10$ degrees gave the best overall performance.

Table \ref{table:params_q_network} shows the neural network setup for the single pendulum problem.
\begin{table}[H]
    \centering
    \begin{tabular}{|
    >{\columncolor[HTML]{CBCEFB}}c |c|c|}
    \hline
    \cellcolor[HTML]{9AFF99}\textbf{Layer} & \cellcolor[HTML]{9AFF99}\textbf{Number of Neurons} & \cellcolor[HTML]{9AFF99}\textbf{Activation} \\ \hline
    \textbf{Input}                         & 4                                                  & -                                           \\ \hline
    \textbf{Hidden 1}                      & 20                                                 & ReLU                                        \\ \hline
    \textbf{Hidden 2}                      & 40                                                 & ReLU                                        \\ \hline
    \textbf{Output}                        & 3                                                  & Linear                                      \\ \hline
    \end{tabular}
    \caption{Network parameters for the Q-network}
    \label{table:params_q_network}
\end{table}

Table \ref{table:params_pendulum} shows the training parameters for the single pendulum problem.
\begin{table}[H]
\centering
\begin{tabular}{|
>{\columncolor[HTML]{FFCE93}}c |c|}
\hline
\textbf{Max Angle} & 10 \\ \hline
\textbf{Step Size} & 0.02 \\ \hline
\textbf{Action Space} & [-10,0,10] \\ \hline
\textbf{Learning Rate} & 0.001 \\ \hline
\textbf{Memory} & 2000 \\ \hline
\textbf{Batch Size} & 32 \\ \hline
\textbf{Discount} & 0.9 \\ \hline
\end{tabular}
\caption{Parameters for the Deep Q-learning algorithm on the single pendulum problem.}
\label{table:params_pendulum}
\end{table}

Figure \ref{fig:single_pendulum_eval} shows the agent performance over episodes.
The agent starts to learn how to move the cart to increase the score after a few episodes.
Eventually the reward gain stops increasing.
The system was simulated with the maximum $200$ steps for each episode, capping the max possible reward to $200$.
However, $200$ is only possible if the initial condition starts in the equilibrium point.
The initial condition for each episode is randomized preventing the system from reaching $200$ score in the evaluation.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\columnwidth]{figures/SinglePendulum_eval.png}
	\caption{Evaluated controller score for every $10$ episodes. The evaluation is measured as the average score of $10$ full controlled simulations. Maximum number of steps for each evaluation iteration was $200$, capping the maximum possible reward to $200$.}
	\label{fig:single_pendulum_eval}
\end{figure}

%Figure \ref{fig:single_pendulum_angle} shows the pendulum angle over time for training episode $170$.
%It seems as if the system is learning which direction to push the cart to get the pendulum closer to the equilibrium point.
%The curve from time $0.5$ to $2.5$ also indicates that the system is starting to understand that it needs to slow down the pendulum before reaching equilibrium to prevent overshooting.
%However, later on it seems to start oscillating around the equilibrium.
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.9\columnwidth]{figures/Pendulum_angle.png}
%	\label{fig:single_pendulum_angle}
%	\caption{Pendulum angle over time for training episode $170$.}
%\end{figure}

The trained controller was used to control the single pendulum for initial conditions outside the trained domain, and the controller managed to bring back the pendulum to the known domain and balance it.
This can be seen in Figure \ref{fig:single_pendulum_outside_training_domain}.
Results indicate that the network has managed to generalize the problem.
This would not be possible with a regular Q-table approach since the agent has never seen the states outside the training domain and would end up performing random actions.
Although the agent was able to bring the pendulum back to its equilibrium point, it later starts to move to the right with a small angle.
The reason for this might be that the cart ended up far away from the origin, which confused the agent.
This can be resolved by training the agent for more episodes.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\columnwidth]{figures/Pendulum_angle_20.png}
	\caption{Changes in pendulum angle $\theta$ over time for controlled simulation of single pendulum on cart problem. The initial condition was $\theta = 20$ degrees.}
	\label{fig:single_pendulum_outside_training_domain}
\end{figure}
Figure \ref{fig:pendulum_gamma_01} and \ref{fig:pendulum_gamma_09} shows the average Q-values during training for $\gamma=0.1$ and $\gamma=0.9$ respectively. It can be seen that the Q-values for a low $\gamma$ value converges much faster while for the larger $\gamma$ value, convergence has not happened at the end of the training.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\columnwidth]{figures/Pendulum_q_gamma_01.png}
	\caption{Q-values for $\gamma=0.1$.}
	\label{fig:pendulum_gamma_01}
\end{figure} 
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\columnwidth]{figures/Pendulum_q_gamma_09.png}
	\caption{Q-values for $\gamma=0.9$.}
	\label{fig:pendulum_gamma_09}
\end{figure}

Figure \ref{fig:single_pendulum_eval_01} shows the training result for $\gamma=0.1$, while the other hyperparameters remained the same. It can be seen that the score converges faster since the Q-values converge faster. This is due to the algorithm only taking the instant and near future rewards in to consideration, thus being short sighted. However the overall performance showed that, with $\gamma=0.1$ the agent was unable to generalize and even had issues with initial conditions that were close to $\theta_{max}$. This can also be seen in the dip during the end of the training in Figure \ref{fig:single_pendulum_eval_01}.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\columnwidth]{figures/pendulum_gamma_01.png}
	\caption{Evaluation of the controller with the same conditions in Table \ref{table:params_pendulum}, except that $\gamma=0.1$. The drop at the end may be caused due to outlier initial conditions during evaluation.}
	\label{fig:single_pendulum_eval_01}
\end{figure}

Figure \ref{fig:pendulum_force} shows the distribution of actions taken for an agent that was trained with a larger action space. This plot shows that most of these actions were rarely used except for $\pm 10$. When the agent was trained for the same number of episodes, the overall performance was worse than in Figure \ref{fig:single_pendulum_eval}. This showed that a larger action space requires more training in order to get the same performance. In this case training time can be reduced by only using the actions that were most used by the agent.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\columnwidth]{figures/Pendulum_force.png}
	\caption{The distribution of used actions for an action space of $[-20,-10,-5,0,5,10,20]$. The model was trained with the same hyperparameters in Table \ref{table:params_pendulum}.}
	\label{fig:pendulum_force}
\end{figure}


% If you need to use tables, Table~\ref{tab:extable} shows an example of how they can be typeset. For further details, see \cite{tablelatexwiki}.

\subsection{Balancing Double Inverted Pendulum}
%Results are currently not sufficient.
%Various experiments are being conducted with different hyper parameters such as network sizes, different action spaces, learning rates etc.
%A problem with tuning hyper parameters is the long feedback loop coming from long training times.

The termination condition is given by equation \eqref{eqn:termination_double_pendulum},
\begin{equation}
	|\theta_1| > \theta_{max} \text{ or } |\theta_2| > \theta_{max}
	\label{eqn:termination_double_pendulum}
\end{equation}
where $\theta_1$ is the angle of the inner pendulum and $\theta_2$ is the angle of the outer pendulum.
$\theta_{max} = 5$ degrees was chosen in order to train the agent in a strict state space close to the equilibrium point.
Due to the limited degrees of freedom of the system higher values of $\theta_{max}$ can allow the pendulums to move to positions where they can not be recovered from.
Reward is given by equation \eqref{eqn:reward_double_pendulum},
\begin{equation}
	R(s) = 1 - \left( \theta_1^2 + \theta_2^2 \right) - \left(\dot{\theta_1}^2 + \dot{\theta_2}^2 \right)
	\label{eqn:reward_double_pendulum}
\end{equation}
where the agent receives a bonus for being alive of $1$, a penalty for being further away from the equilibrium point given by $\theta_1^2 + \theta_2^2$, and a penalty for the velocity of the pendulums given by $\dot{\theta_1}^2 + \dot{\theta_2}^2$.
The total score received in an episode depends on the step size of the simulation and the duration that the agent is able to survive.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\columnwidth]{figures/double_pendulum_eval.png}
	\caption{Current results for training the controller.}
	\label{fig:double_pendulum_score}
\end{figure}
Although this method works for the single pendulum case, since the double pendulum is a highly chaotic system, the algorithm is having a difficult time training the controller as seen in Figure \ref{fig:double_pendulum_score}.
We believe that the reason for this could be that the hyper parameters are not optimal or we are not giving the algorithm enough time to train even though the training takes several hours.

Table \ref{table:double_params_q_network} shows the neural network setup for the single pendulum problem.
\begin{table}[H]
    \centering
    \begin{tabular}{|
    >{\columncolor[HTML]{CBCEFB}}c |c|c|}
    \hline
    \cellcolor[HTML]{9AFF99}\textbf{Layer} & \cellcolor[HTML]{9AFF99}\textbf{Number of Neurons} & \cellcolor[HTML]{9AFF99}\textbf{Activation} \\ \hline
    \textbf{Input}                         & 4                                                  & -                                           \\ \hline
    \textbf{Hidden 1}                      & 20                                                 & ReLU                                        \\ \hline
    \textbf{Hidden 2}                      & 40                                                 & ReLU                                        \\ \hline
    \textbf{Output}                        & 3                                                  & Linear                                      \\ \hline
    \end{tabular}
    \caption{Network parameters for the Q-network}
    \label{table:double_params_q_network}
\end{table}

Table \ref{table:params_double_pendulum} shows the training parameters for the single pendulum problem.
\begin{table}[H]
\centering
\begin{tabular}{|
>{\columncolor[HTML]{FFCE93}}c |c|}
\hline
\textbf{Max Angle} & 10 \\ \hline
\textbf{Step Size} & 0.02 \\ \hline
\textbf{Action Space} & [-10,0,10] \\ \hline
\textbf{Learning Rate} & 0.001 \\ \hline
\textbf{Memory} & 2000 \\ \hline
\textbf{Batch Size} & 32 \\ \hline
\textbf{Discount} & 0.9 \\ \hline
\end{tabular}
\caption{Parameters for the Deep Q-learning algorithm on the single pendulum problem.}
\label{table:params_double_pendulum}
\end{table}
\subsection{Training Time}
The training time of the agent heavily depends on the hyperparameters and the overall performance of the agent. 
In the first few episodes, due to the agents lack of experience the episodes will terminate faster, thus resulting in shorter episodes.
As the agent gains more experience, it learns to survive for longer, increasing the computation time for a single episode.
Table \ref{table:params_pendulum} and \ref{table:other_params_time} show the computation and simulation time differences for a change in the termination condition.
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
\rowcolor[HTML]{CBCEFB} 
\multicolumn{3}{|c|}{\cellcolor[HTML]{CBCEFB}\textbf{Single Pendulum}} \\ \hline
\rowcolor[HTML]{FFCCC9} 
\textbf{Episode} & \textbf{Computation} & \textbf{Simulation} \\ \hline
\rowcolor[HTML]{FFFFC7} 
\textbf{1} & 0.72 s & 0.56 s\\ \hline
\rowcolor[HTML]{FFFFC7} 
\vdots & \vdots & \vdots \\ \hline
\rowcolor[HTML]{FFFFC7} 
\textbf{50} & 24.73 s & 4 s \\ \hline
\rowcolor[HTML]{FFFFC7} 
\vdots & \vdots & \vdots \\ \hline
\rowcolor[HTML]{FFFFC7} 
\textbf{100} & 28.52 s & 4 s\\ \hline
\end{tabular}
\caption{Computation and simulation times for episodes with hyperparameters specified in Table \ref{table:params_pendulum}.}
\label{table:def_params_time}
\end{table}
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
\rowcolor[HTML]{CBCEFB} 
\multicolumn{3}{|c|}{\cellcolor[HTML]{CBCEFB}\textbf{Single Pendulum}} \\ \hline
\rowcolor[HTML]{FFCCC9} 
\textbf{Episode} & \textbf{Computation} & \textbf{Simulation} \\ \hline
\rowcolor[HTML]{FFFFC7} 
\textbf{1} & 0.76 s & 0.6 s \\ \hline
\rowcolor[HTML]{FFFFC7} 
\vdots & \vdots & \vdots \\ \hline
\rowcolor[HTML]{FFFFC7} 
\textbf{50} & 6.03 s & 0.86 s \\ \hline
\rowcolor[HTML]{FFFFC7} 
\vdots & \vdots & \vdots \\ \hline
\rowcolor[HTML]{FFFFC7} 
\textbf{100} & 26.77 s & 4 s\\ \hline
\end{tabular}
\caption{Computation and simulation times for episodes with hyperparameters specified in Table \ref{table:params_pendulum}, with a modification of $\theta_{max} = 20$ degrees.}
\label{table:other_params_time}
\end{table}

\section{Discussion}
% Talk about the problem
The double pendulum is a highly chaotic system and with the cart limited to a single degree of freedom it is difficult to control.
In situations where the agent receives two states that are very close to each other, the applied action may result in very different outcomes which makes the learning of the expected reward for that action very difficult.
% add difference between wide and deep networks
We have found that deeper networks perform better than wider ones for this problem.
This may be due to wider networks having less non-linearity compared to deeper ones since deep networks have more coupled activations.

After playing around with the exploration rate $\epsilon$, we have found that a dynamic exploration rate performs better for this problem, rather than a static one.
Higher exploration rate forces the agent to take actions that it normally would avoid causing it to explore the action space more.
For Q-learning, this is a desired thing since the purpose is to fill and update the Q-table.
However a high exploration rate leads to the agent exploring things that it should avoid, rather than finding the best action to perform.
In Q-learning, this is not an issue since the q-values of the state-action pairs do not directly affect each other.
However for Deep Q-learning, every weight update affects all q-values for the state-action pairs.
In the case where the learning rate is too low, the agent will almost always prefer to take the same actions which can cause overfitting in the network.
This is why we preferred to train the agent with a dynamic exploration rate, where the agent performs randomly in the beginning and over time becomes more deterministic.
However we still have a minimum $\epsilon$, to allow the agent to explore during the later episodes of training.
This prevents the agent from overfitting to states close to the equilibrium point.
For the same reason, we preferred to use a strict termination condition. 
Since the agents purpose is to control the pendulums around the equilibrium, there is little to no advantage in allowing the pendulum to move too far from the equilibrium point.

When we used a discount factor closer to $1$, we found that it takes longer for the q-values to converge, and they converged to larger values, as seen in Figure \ref{fig:pendulum_gamma_09}.
Although this specific problem does not have any delayed rewards, such as reaching a goal in a game, it made more sense to use a high discount factor $\gamma$, so that the agent can take into account the consequences of its actions, such as if an action makes the pendulum overshoot the equilibrium point or not.
Our results indicated that using a higher $\gamma$ value helped the agent generalize much better and was more resilient to near edge starting conditions.

% Difficulties in training
As seen in Table \ref{table:def_params_time} tuning hyperparameters is a difficult process. 
Due to the time the training takes, evaluating the performance of a different set of hyperparameters is challenging.
Since the training process highly depends on the hyperparameters, the time it takes for one episode can be vastly different for different hyperparameters.
Aside from the time it takes to get the results, evaluating the effect of the hyperparameters is not straight forward.
Even when using the same set of hyperparameters we can get different results.
Trying to estimate the effect of a new hyperparameter is problematic.
Due to these reasons, a trial and error approach was more effective.
We believe that with the right hyperparameters, the double pendulum can be controlled.

The duration that the agent survives for an episode does not directly correlate with the agents performance.
Different hyperparameters, reward functions and termination conditions affect the duration the agent is able to survive, which makes evaluating hyperparameters problematic.
As seen in Table \ref{table:def_params_time} and \ref{table:other_params_time}, the agent was trained with a more flexlible termination condition, which allowed it to survive longer in an episode, however it takes more episodes for the Q-values to converge since the state-action space is much larger than before which takes more training time.
The simulation time for episode $50$ in Table \ref{table:def_params_time} cannot be directly compared with the corresponding value in Table \ref{table:other_params_time}.

We have tried other model-free methods such as Linear Q-learning \cite{Linear_Q_Learning} and Double Deep Q-learning \cite{Deep_Double_Q_Learning}, but they did not achieve better performance than our Deep Q-learning \cite{Deep_Q_Learning_First} implementation.

% pendulum specific
% Double pendulum specific
% Future work
%While training a controller for the double pendulum problem, we saw that for some cases after the average score increases for a while, it suddenly starts to decrease and can remain there untill the training ends.
%We think that this may be a result of the hyper parameter choices such as network architecture.
%We believe that this may be an effect such as overtraining or that the network is too simple.
%We are trying different combinations in order to understand this behavior.
%Also, as seen in Figure \ref{fig:double_pendulum_score}, we are getting sudden spikes during training.
%We are also currently trying to understand this phenomenon.
%\section{Project Plan}
%The initial project plan was followed quite well.
%However, the time to tune hyper parameters for the double pendulum problem were underestimated.
%Work on that part still remains.
%One problem for tuning the hyper parameters is that the feedback loop for a single change is long.
%It takes a long time to train the controller for the double pendulum and therefore a long time before hyper parameters can be adjusted.
%Figure \ref{fig:gantt_chart} shows the updated Gantt chart.
%The green colored blocks are completed steps.
%Blue blocks are work in progress.
%\begin{figure}[H]
%   \centering
%   \includegraphics[width=0.9\columnwidth]{figures/Revised_Project_plan.png}
%   \caption{Revised Gantt chart.}
%   \label{fig:gantt_chart}
%\end{figure}
% Prints cited references
\printbibliography
\end{document}
